{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Observation: 1\n",
      "Step 1: Stimulus 0\n",
      "Current Observation: 0\n",
      "Step 2: Stimulus 3\n",
      "Current Observation: 3\n",
      "Step 3: Stimulus 0\n",
      "Current Observation: 0\n",
      "Step 4: Stimulus 2\n",
      "Current Observation: 2\n",
      "Step 5: Stimulus 0\n",
      "Current Observation: 0\n",
      "Step 6: Stimulus 1\n",
      "Current Observation: 1\n",
      "Episode finished.\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "\n",
    "class DelaySampleToMatchEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "    \n",
    "    def __init__(self, n_stimuli=3, delay_length=1):\n",
    "        super(DelaySampleToMatchEnv, self).__init__()\n",
    "        \n",
    "        # Number of different stimuli (excluding the delay)\n",
    "        self.n_stimuli = n_stimuli\n",
    "        \n",
    "        # Length of delay between stimuli\n",
    "        self.delay_length = delay_length\n",
    "        \n",
    "        # Action space and observation space\n",
    "        # Stimuli are represented as integers from 0 to n_stimuli (0 is the delay)\n",
    "        self.action_space = spaces.Discrete(n_stimuli + 1)\n",
    "        self.observation_space = spaces.Discrete(n_stimuli + 1)\n",
    "        \n",
    "        # Initialize the sequence and current step\n",
    "        self.sequence = []\n",
    "        self.current_step = 0\n",
    "        \n",
    "    def _generate_sequence(self):\n",
    "        # Randomly select n stimuli (1 to n_stimuli, excluding 0 which is the delay)\n",
    "        stimuli = np.random.choice(range(1, self.n_stimuli + 1), self.n_stimuli, replace=False)\n",
    "        # Create the sequence: stimulus1-delay-stimulus2-delay-...-stimulusN-stimulus1\n",
    "        sequence = []\n",
    "        for stimulus in stimuli:\n",
    "            sequence.append(stimulus)\n",
    "            sequence.extend([0] * self.delay_length)  # 0 represents the delay\n",
    "        sequence.append(stimuli[0])\n",
    "        return sequence\n",
    "        \n",
    "    def reset(self):\n",
    "        # Generate a new sequence for the episode\n",
    "        self.sequence = self._generate_sequence()\n",
    "        self.current_step = 0\n",
    "        # Return the first stimulus\n",
    "        return self.sequence[self.current_step]\n",
    "    \n",
    "    def step(self, action):\n",
    "        self.current_step += 1\n",
    "        \n",
    "        # Check if the episode is done\n",
    "        done = self.current_step >= len(self.sequence)\n",
    "        \n",
    "        # Get the next observation\n",
    "        if not done:\n",
    "            observation = self.sequence[self.current_step]\n",
    "        else:\n",
    "            observation = None\n",
    "        \n",
    "        # In this environment, we don't have a reward structure as it's a sequence task\n",
    "        reward = 0\n",
    "        \n",
    "        return observation, reward, done, {}\n",
    "    \n",
    "    def render(self, mode='human'):\n",
    "        if self.current_step < len(self.sequence):\n",
    "            print(f\"Step {self.current_step}: Stimulus {self.sequence[self.current_step]}\")\n",
    "        else:\n",
    "            print(\"Episode finished.\")\n",
    "\n",
    "# Example usage\n",
    "env = DelaySampleToMatchEnv(n_stimuli=3, delay_length=1)\n",
    "observation = env.reset()\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    print(\"Current Observation:\", observation)\n",
    "    action = env.action_space.sample()  # Random action (not used in this environment)\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    env.render()\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Observation: 1\n",
      "Step 1: Stimulus 0\n",
      "Action: 3, Reward: 0\n",
      "Current Observation: 0\n",
      "Step 2: Stimulus 2\n",
      "Action: 3, Reward: -1\n",
      "Current Observation: 2\n",
      "Step 3: Stimulus 0\n",
      "Action: 3, Reward: 0\n",
      "Current Observation: 0\n",
      "Step 4: Stimulus 3\n",
      "Action: 2, Reward: -1\n",
      "Current Observation: 3\n",
      "Step 5: Stimulus 0\n",
      "Action: 0, Reward: 0\n",
      "Current Observation: 0\n",
      "Step 6: Stimulus 1\n",
      "Action: 1, Reward: -1\n",
      "Current Observation: 1\n",
      "Episode finished.\n",
      "Action: 1, Reward: 1\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "\n",
    "class DelaySampleToMatchEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "    \n",
    "    def __init__(self, n_stimuli=3, delay_length=1):\n",
    "        super(DelaySampleToMatchEnv, self).__init__()\n",
    "        \n",
    "        # Number of different stimuli (excluding the delay)\n",
    "        self.n_stimuli = n_stimuli\n",
    "        \n",
    "        # Length of delay between stimuli\n",
    "        self.delay_length = delay_length\n",
    "        \n",
    "        # Action space and observation space\n",
    "        # Stimuli are represented as integers from 0 to n_stimuli (0 is the delay)\n",
    "        self.action_space = spaces.Discrete(n_stimuli + 1)\n",
    "        self.observation_space = spaces.Discrete(n_stimuli + 1)\n",
    "        \n",
    "        # Initialize the sequence and current step\n",
    "        self.sequence = []\n",
    "        self.current_step = 0\n",
    "        self.first_stimulus = None\n",
    "        \n",
    "    def _generate_sequence(self):\n",
    "        # Randomly select n stimuli (1 to n_stimuli, excluding 0 which is the delay)\n",
    "        stimuli = np.random.choice(range(1, self.n_stimuli + 1), self.n_stimuli, replace=False)\n",
    "        self.first_stimulus = stimuli[0]\n",
    "        # Create the sequence: stimulus1-delay-stimulus2-delay-...-stimulusN-stimulus1\n",
    "        sequence = []\n",
    "        for stimulus in stimuli:\n",
    "            sequence.append(stimulus)\n",
    "            sequence.extend([0] * self.delay_length)  # 0 represents the delay\n",
    "        sequence.append(stimuli[0])\n",
    "        return sequence\n",
    "        \n",
    "    def reset(self):\n",
    "        # Generate a new sequence for the episode\n",
    "        self.sequence = self._generate_sequence()\n",
    "        self.current_step = 0\n",
    "        # Return the first stimulus\n",
    "        return self.sequence[self.current_step]\n",
    "    \n",
    "    def step(self, action):\n",
    "        reward = 0\n",
    "        done = False\n",
    "        \n",
    "        # If the action is non-zero and the current stimulus is a delay, give a reward of -1\n",
    "        if action != 0 and self.sequence[self.current_step] == 0:\n",
    "            reward = -1\n",
    "        \n",
    "        # If the current step is the last one, check if the action matches the first stimulus\n",
    "        if self.current_step == len(self.sequence) - 1:\n",
    "            done = True\n",
    "            if action == self.first_stimulus:\n",
    "                reward = 1\n",
    "            else:\n",
    "                reward = -1\n",
    "        \n",
    "        # Advance to the next step\n",
    "        self.current_step += 1\n",
    "        \n",
    "        # Check if the episode is done\n",
    "        done = self.current_step >= len(self.sequence)\n",
    "        \n",
    "        # Get the next observation\n",
    "        if not done:\n",
    "            observation = self.sequence[self.current_step]\n",
    "        else:\n",
    "            observation = None\n",
    "        \n",
    "        return observation, reward, done, {}\n",
    "    \n",
    "    def render(self, mode='human'):\n",
    "        if self.current_step < len(self.sequence):\n",
    "            print(f\"Step {self.current_step}: Stimulus {self.sequence[self.current_step]}\")\n",
    "        else:\n",
    "            print(\"Episode finished.\")\n",
    "\n",
    "# Example usage\n",
    "env = DelaySampleToMatchEnv(n_stimuli=3, delay_length=1)\n",
    "observation = env.reset()\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    print(\"Current Observation:\", observation)\n",
    "    action = env.action_space.sample()  # Random action (for demonstration purposes)\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    env.render()\n",
    "    print(f\"Action: {action}, Reward: {reward}\")\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "int() argument must be a string, a bytes-like object or a number, not 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 128\u001b[0m\n\u001b[1;32m    126\u001b[0m action, hidden \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mchoose_action(state, hidden)  \u001b[38;5;66;03m# Choose action based on epsilon-greedy policy\u001b[39;00m\n\u001b[1;32m    127\u001b[0m next_state, reward, done, info \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)  \u001b[38;5;66;03m# Take the action\u001b[39;00m\n\u001b[0;32m--> 128\u001b[0m next_state \u001b[38;5;241m=\u001b[39m \u001b[43mone_hot_encode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservation_space\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    129\u001b[0m agent\u001b[38;5;241m.\u001b[39mstore_transition(state, action, reward, next_state, done)\n\u001b[1;32m    130\u001b[0m agent\u001b[38;5;241m.\u001b[39mlearn()  \u001b[38;5;66;03m# Update Q-network\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 105\u001b[0m, in \u001b[0;36mone_hot_encode\u001b[0;34m(state, state_space)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(state, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    104\u001b[0m     state \u001b[38;5;241m=\u001b[39m state[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# Extract the state from the tuple if reset() returns a tuple\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m one_hot[\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# Ensure state is converted to integer\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m one_hot\n",
      "\u001b[0;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a number, not 'NoneType'"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# RNN for approximating Q-values\n",
    "class RNNQNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(RNNQNetwork, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.i2h = nn.Linear(input_dim, hidden_dim)\n",
    "        self.h2h = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.h2o = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        hidden = torch.tanh(self.i2h(x) + self.h2h(hidden))\n",
    "        output = self.h2o(hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(batch_size, self.hidden_dim)\n",
    "\n",
    "\n",
    "# Deep Q-Learning Agent with RNN\n",
    "class DQNAgent:\n",
    "    def __init__(self, env, hidden_dim=64, learning_rate=0.001, discount_factor=0.99, epsilon=1.0, epsilon_min=0.01, epsilon_decay=0.995, buffer_size=10000, batch_size=64):\n",
    "        self.env = env\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.buffer_size = buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.q_network = RNNQNetwork(env.observation_space.n, hidden_dim, env.action_space.n).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=learning_rate)\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def choose_action(self, state, hidden):\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            q_values, hidden = self.q_network(state_tensor, hidden)\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return self.env.action_space.sample(), hidden  # Explore\n",
    "        else:\n",
    "            return torch.argmax(q_values).item(), hidden  # Exploit\n",
    "\n",
    "    def store_transition(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def learn(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        # Convert lists to numpy arrays and concatenate\n",
    "        states = np.array(states, dtype=np.float32)\n",
    "        actions = np.array(actions, dtype=np.int64)\n",
    "        rewards = np.array(rewards, dtype=np.float32)\n",
    "        next_states = np.array(next_states, dtype=np.float32)\n",
    "        dones = np.array(dones, dtype=np.float32)\n",
    "\n",
    "        # Convert concatenated numpy arrays to PyTorch tensors\n",
    "        states_tensor = torch.tensor(states).to(self.device)\n",
    "        actions_tensor = torch.tensor(actions).unsqueeze(-1).to(self.device)\n",
    "        rewards_tensor = torch.tensor(rewards).unsqueeze(-1).to(self.device)\n",
    "        next_states_tensor = torch.tensor(next_states).to(self.device)\n",
    "        dones_tensor = torch.tensor(dones).unsqueeze(-1).to(self.device)\n",
    "\n",
    "        # Initialize the hidden state for the first batch item\n",
    "        hidden = self.q_network.init_hidden(self.batch_size).to(self.device)\n",
    "\n",
    "        current_q_values, _ = self.q_network(states_tensor, hidden)\n",
    "        current_q_values = current_q_values.gather(1, actions_tensor)\n",
    "        next_q_values, _ = self.q_network(next_states_tensor, hidden)\n",
    "        next_q_values = next_q_values.max(1)[0].unsqueeze(-1)\n",
    "        target_q_values = rewards_tensor + self.discount_factor * next_q_values * (1 - dones_tensor)\n",
    "\n",
    "        loss = self.criterion(current_q_values, target_q_values)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "\n",
    "\n",
    "# Function to one-hot encode state\n",
    "def one_hot_encode(state, state_space):\n",
    "    one_hot = np.zeros(state_space)\n",
    "    if isinstance(state, tuple):\n",
    "        state = state[0]  # Extract the state from the tuple if reset() returns a tuple\n",
    "    one_hot[int(state)] = 1  # Ensure state is converted to integer\n",
    "    return one_hot\n",
    "\n",
    "# Create the environment\n",
    "#env = gym.make('FrozenLake-v1', desc=None, map_name=\"4x4\", is_slippery=False)\n",
    "\n",
    "# Instantiate the agent\n",
    "agent = DQNAgent(env)\n",
    "\n",
    "n_episodes = 2500\n",
    "win_pct_list = []\n",
    "scores = []\n",
    "\n",
    "# Training loop\n",
    "for i in range(n_episodes):\n",
    "    state = agent.env.reset()  # Reset the environment\n",
    "    state = one_hot_encode(state, env.observation_space.n)\n",
    "    done = False\n",
    "    score = 0\n",
    "    while not done:\n",
    "        hidden = agent.q_network.init_hidden(1).to(agent.device)\n",
    "        action, hidden = agent.choose_action(state, hidden)  # Choose action based on epsilon-greedy policy\n",
    "        next_state, reward, done, info = agent.env.step(action)  # Take the action\n",
    "        next_state = one_hot_encode(next_state, env.observation_space.n)\n",
    "        agent.store_transition(state, action, reward, next_state, done)\n",
    "        agent.learn()  # Update Q-network\n",
    "        state = next_state  # Move to the next state\n",
    "        score += reward\n",
    "    scores.append(score)\n",
    "    if i % 100 == 0:\n",
    "        avg_score = np.mean(scores[-100:])\n",
    "        win_pct_list.append(avg_score)\n",
    "        print('episode', i, 'win pct %.2f' % avg_score, 'epsilon %.2f' % agent.epsilon)\n",
    "\n",
    "# Plotting the win percentage over episodes\n",
    "plt.plot(win_pct_list)\n",
    "plt.xlabel('Episodes (x100)')\n",
    "plt.ylabel('Win Percentage')\n",
    "plt.title('Win Percentage over Time')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
