{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from environment import DelaySampleToMatchEnv\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 0 win pct -16.00 epsilon 1.00\n",
      "episode 100 win pct -1.98 epsilon 0.01\n",
      "episode 200 win pct -0.98 epsilon 0.01\n",
      "episode 300 win pct -0.92 epsilon 0.01\n",
      "episode 400 win pct -0.99 epsilon 0.01\n",
      "episode 500 win pct -1.03 epsilon 0.01\n",
      "episode 600 win pct -0.92 epsilon 0.01\n",
      "episode 700 win pct -1.01 epsilon 0.01\n",
      "episode 800 win pct -1.08 epsilon 0.01\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 140\u001b[0m\n\u001b[1;32m    138\u001b[0m next_state \u001b[38;5;241m=\u001b[39m one_hot_encode(next_state, env\u001b[38;5;241m.\u001b[39mobservation_space\u001b[38;5;241m.\u001b[39mn)\n\u001b[1;32m    139\u001b[0m agent\u001b[38;5;241m.\u001b[39mstore_transition(state, action, reward, next_state, done)\n\u001b[0;32m--> 140\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Update Q-network\u001b[39;00m\n\u001b[1;32m    141\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state  \u001b[38;5;66;03m# Move to the next state\u001b[39;00m\n\u001b[1;32m    142\u001b[0m score \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "Cell \u001b[0;32mIn[4], line 103\u001b[0m, in \u001b[0;36mDQNAgent.learn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    100\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion(current_q_values, target_q_values)\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 103\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepsilon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepsilon_min, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepsilon \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepsilon_decay)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# RNN for approximating Q-values with positional embeddings\n",
    "class RNNQNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, max_seq_len):\n",
    "        super(RNNQNetwork, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.input_dim = input_dim\n",
    "        \n",
    "        # Define the positional embeddings\n",
    "        self.positional_embeddings = nn.Embedding(max_seq_len, input_dim)\n",
    "        \n",
    "        self.i2h = nn.Linear(input_dim, hidden_dim)\n",
    "        self.h2h = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.h2o = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, hidden, position):\n",
    "        # Add positional embeddings to the input\n",
    "        pos_embedding = self.positional_embeddings(position)\n",
    "        x = x + pos_embedding\n",
    "        \n",
    "        hidden = torch.tanh(self.i2h(x) + self.h2h(hidden))\n",
    "        output = self.h2o(hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(batch_size, self.hidden_dim)\n",
    "\n",
    "\n",
    "# Deep Q-Learning Agent with RNN and positional embeddings\n",
    "class DQNAgent:\n",
    "    def __init__(self, env, hidden_dim=64, learning_rate=0.001, discount_factor=0.99, epsilon=1.0, epsilon_min=0.01, epsilon_decay=0.995, buffer_size=10000, batch_size=64):\n",
    "        self.env = env\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.buffer_size = buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.q_network = RNNQNetwork(env.observation_space.n, hidden_dim, env.action_space.n, max_seq_len=100).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=learning_rate)\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def choose_action(self, state, hidden, position):\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(self.device)\n",
    "        position_tensor = torch.tensor([position], dtype=torch.long).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            q_values, hidden = self.q_network(state_tensor, hidden, position_tensor)\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return self.env.action_space.sample(), hidden  # Explore\n",
    "        else:\n",
    "            return torch.argmax(q_values).item(), hidden  # Exploit\n",
    "\n",
    "    def store_transition(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def learn(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        # Convert lists to numpy arrays and concatenate\n",
    "        states = np.array(states, dtype=np.float32)\n",
    "        actions = np.array(actions, dtype=np.int64)\n",
    "        rewards = np.array(rewards, dtype=np.float32)\n",
    "        next_states = np.array(next_states, dtype=np.float32)\n",
    "        dones = np.array(dones, dtype=np.float32)\n",
    "\n",
    "        # Convert concatenated numpy arrays to PyTorch tensors\n",
    "        states_tensor = torch.tensor(states).to(self.device)\n",
    "        actions_tensor = torch.tensor(actions).unsqueeze(-1).to(self.device)\n",
    "        rewards_tensor = torch.tensor(rewards).unsqueeze(-1).to(self.device)\n",
    "        next_states_tensor = torch.tensor(next_states).to(self.device)\n",
    "        dones_tensor = torch.tensor(dones).unsqueeze(-1).to(self.device)\n",
    "\n",
    "        # Initialize the hidden state for the first batch item\n",
    "        hidden = self.q_network.init_hidden(self.batch_size).to(self.device)\n",
    "        position = torch.arange(states_tensor.size(0)).to(self.device)\n",
    "\n",
    "        current_q_values, _ = self.q_network(states_tensor, hidden, position)\n",
    "        current_q_values = current_q_values.gather(1, actions_tensor)\n",
    "        next_q_values, _ = self.q_network(next_states_tensor, hidden, position)\n",
    "        next_q_values = next_q_values.max(1)[0].unsqueeze(-1)\n",
    "        target_q_values = rewards_tensor + self.discount_factor * next_q_values * (1 - dones_tensor)\n",
    "\n",
    "        loss = self.criterion(current_q_values, target_q_values)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "\n",
    "# Function to one-hot encode state\n",
    "def one_hot_encode(state, state_space):\n",
    "    one_hot = np.zeros(state_space)\n",
    "    if isinstance(state, tuple):\n",
    "        state = state[0]  # Extract the state from the tuple if reset() returns a tuple\n",
    "    one_hot[int(state)] = 1  # Ensure state is converted to integer\n",
    "    return one_hot\n",
    "\n",
    "# Create the environment\n",
    "env = DelaySampleToMatchEnv(n_stimuli=15, delay_length=1)\n",
    "\n",
    "# Instantiate the agent\n",
    "agent = DQNAgent(env, hidden_dim=128)\n",
    "\n",
    "n_episodes = 10000\n",
    "win_pct_list = []\n",
    "scores = []\n",
    "\n",
    "# Training loop\n",
    "for i in range(n_episodes):\n",
    "    state = agent.env.reset()  # Reset the environment\n",
    "    state = one_hot_encode(state, env.observation_space.n)\n",
    "    done = False\n",
    "    score = 0\n",
    "    position = 0\n",
    "    hidden = agent.q_network.init_hidden(1).to(agent.device)\n",
    "    while not done:\n",
    "        action, hidden = agent.choose_action(state, hidden, position)  # Choose action based on epsilon-greedy policy\n",
    "        next_state, reward, done, info = agent.env.step(action)  # Take the action\n",
    "        next_state = one_hot_encode(next_state, env.observation_space.n)\n",
    "        agent.store_transition(state, action, reward, next_state, done)\n",
    "        agent.learn()  # Update Q-network\n",
    "        state = next_state  # Move to the next state\n",
    "        score += reward\n",
    "        position += 1\n",
    "    scores.append(score)\n",
    "    if i % 100 == 0:\n",
    "        avg_score = np.mean(scores[-100:])\n",
    "        win_pct_list.append(avg_score)\n",
    "        print('episode', i, 'win pct %.2f' % avg_score, 'epsilon %.2f' % agent.epsilon)\n",
    "\n",
    "# Plotting the win percentage over episodes\n",
    "plt.plot(win_pct_list)\n",
    "plt.xlabel('Episodes (x100)')\n",
    "plt.ylabel('Win Percentage')\n",
    "plt.title('Win Percentage over Time')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
