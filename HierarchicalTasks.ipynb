{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8dbb09a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'neurogym'...\n",
      "remote: Enumerating objects: 11100, done.\u001b[K\n",
      "remote: Counting objects: 100% (273/273), done.\u001b[K\n",
      "remote: Compressing objects: 100% (105/105), done.\u001b[K\n",
      "remote: Total 11100 (delta 132), reused 234 (delta 115), pack-reused 10827\u001b[K\n",
      "Receiving objects: 100% (11100/11100), 8.20 MiB | 5.67 MiB/s, done.\n",
      "Resolving deltas: 100% (8318/8318), done.\n",
      "/home/maria/Neuromatch2024/neurogym\n",
      "Obtaining file:///home/maria/Neuromatch2024/neurogym\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy in /home/maria/anaconda3/lib/python3.9/site-packages (from neurogym==0.0.2) (1.23.5)\n",
      "Requirement already satisfied: gym<0.25,>=0.20.0 in /home/maria/anaconda3/lib/python3.9/site-packages (from neurogym==0.0.2) (0.24.1)\n",
      "Requirement already satisfied: matplotlib in /home/maria/anaconda3/lib/python3.9/site-packages (from neurogym==0.0.2) (3.7.1)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/maria/anaconda3/lib/python3.9/site-packages (from gym<0.25,>=0.20.0->neurogym==0.0.2) (2.0.0)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /home/maria/anaconda3/lib/python3.9/site-packages (from gym<0.25,>=0.20.0->neurogym==0.0.2) (0.0.8)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in /home/maria/anaconda3/lib/python3.9/site-packages (from gym<0.25,>=0.20.0->neurogym==0.0.2) (6.7.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/maria/anaconda3/lib/python3.9/site-packages (from matplotlib->neurogym==0.0.2) (1.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/maria/anaconda3/lib/python3.9/site-packages (from matplotlib->neurogym==0.0.2) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/maria/anaconda3/lib/python3.9/site-packages (from matplotlib->neurogym==0.0.2) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/maria/anaconda3/lib/python3.9/site-packages (from matplotlib->neurogym==0.0.2) (1.4.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/maria/anaconda3/lib/python3.9/site-packages (from matplotlib->neurogym==0.0.2) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/maria/anaconda3/lib/python3.9/site-packages (from matplotlib->neurogym==0.0.2) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/maria/anaconda3/lib/python3.9/site-packages (from matplotlib->neurogym==0.0.2) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/maria/anaconda3/lib/python3.9/site-packages (from matplotlib->neurogym==0.0.2) (2.8.2)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /home/maria/anaconda3/lib/python3.9/site-packages (from matplotlib->neurogym==0.0.2) (5.12.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/maria/anaconda3/lib/python3.9/site-packages (from importlib-metadata>=4.8.0->gym<0.25,>=0.20.0->neurogym==0.0.2) (3.15.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/maria/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib->neurogym==0.0.2) (1.16.0)\n",
      "Installing collected packages: neurogym\n",
      "  Running setup.py develop for neurogym\n",
      "Successfully installed neurogym-0.0.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! git clone https://github.com/gyyang/neurogym.git\n",
    "%cd neurogym/\n",
    "! pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89e4f421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium in /home/maria/anaconda3/lib/python3.9/site-packages (0.29.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /home/maria/anaconda3/lib/python3.9/site-packages (from gymnasium) (1.23.5)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/maria/anaconda3/lib/python3.9/site-packages (from gymnasium) (2.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /home/maria/anaconda3/lib/python3.9/site-packages (from gymnasium) (4.12.1)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /home/maria/anaconda3/lib/python3.9/site-packages (from gymnasium) (0.0.4)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in /home/maria/anaconda3/lib/python3.9/site-packages (from gymnasium) (6.7.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/maria/anaconda3/lib/python3.9/site-packages (from importlib-metadata>=4.8.0->gymnasium) (3.15.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install gymnasium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "041a98c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boom Discrete(3) Box(-1.0, [1. 2.], (2,), float32)\n",
      "Tuple(Discrete(3), Box(-1.0, [1. 2.], (2,), float32))\n",
      "boom tensor([0], device='cuda:0') tensor([[-0.9291, -1.5587]], device='cuda:0')\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'list' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 238\u001b[0m\n\u001b[1;32m    235\u001b[0m policy \u001b[38;5;241m=\u001b[39m MLPPolicy(obs_space, act_space[\u001b[38;5;241m0\u001b[39m],act_space[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    236\u001b[0m agent \u001b[38;5;241m=\u001b[39m PPOAgent(policy)\n\u001b[0;32m--> 238\u001b[0m episode_rewards \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[1;32m    241\u001b[0m state \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n",
      "Cell \u001b[0;32mIn[24], line 203\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(env, agent, num_episodes, batch_size)\u001b[0m\n\u001b[1;32m    200\u001b[0m action, log_prob, value \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39mact(state)\n\u001b[1;32m    201\u001b[0m \u001b[38;5;66;03m#action = action.item()  # Convert action tensor to scalar\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;66;03m#print(action)\u001b[39;00m\n\u001b[0;32m--> 203\u001b[0m next_state, reward, done, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m next_state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(next_state)\u001b[38;5;241m.\u001b[39mto(agent\u001b[38;5;241m.\u001b[39mdevice)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    206\u001b[0m memory\u001b[38;5;241m.\u001b[39mappend((state, action, log_prob\u001b[38;5;241m.\u001b[39mitem(), reward, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mint\u001b[39m(done), value))\n",
      "Cell \u001b[0;32mIn[24], line 53\u001b[0m, in \u001b[0;36mMultiTaskEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     51\u001b[0m obs1, reward1, done1, info1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask1\u001b[38;5;241m.\u001b[39mstep(action1\u001b[38;5;241m.\u001b[39mitem())  \u001b[38;5;66;03m# Convert action1 tensor to scalar\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Step task2 with action2 (Box)\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m obs2, reward2, done2, info2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtask2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Convert action2 tensor to list\u001b[39;00m\n\u001b[1;32m     55\u001b[0m concatenated_obs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate([obs1, obs2])\n\u001b[1;32m     56\u001b[0m reward \u001b[38;5;241m=\u001b[39m reward1 \u001b[38;5;241m+\u001b[39m reward2\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/gym/wrappers/order_enforcing.py:37\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Neuromatch2024/neurogym/neurogym/core.py:188\u001b[0m, in \u001b[0;36mTrialEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m    187\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Public interface for the environment.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 188\u001b[0m     ob, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnew_trial\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m info:\n\u001b[1;32m    191\u001b[0m         info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnew_trial\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/Neuromatch2024/neurogym/neurogym/envs/reachingdelayresponse.py:83\u001b[0m, in \u001b[0;36mReachingDelayResponse._step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     80\u001b[0m gt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgt_now  \u001b[38;5;66;03m# 2 dim now\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_period(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstimulus\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m---> 83\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43maction\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m:\n\u001b[1;32m     84\u001b[0m         new_trial \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mabort\n\u001b[1;32m     85\u001b[0m         reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrewards[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mabort\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mTypeError\u001b[0m: '<' not supported between instances of 'list' and 'int'"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import neurogym as ngym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Create a custom multi-task environment\n",
    "import gym\n",
    "import neurogym as ngym\n",
    "import numpy as np\n",
    "\n",
    "class MultiTaskEnv(gym.Env):\n",
    "    def __init__(self, task1_id, task2_id, switch_interval):\n",
    "        super(MultiTaskEnv, self).__init__()\n",
    "        self.task1 = ngym.make(task1_id)\n",
    "        self.task2 = ngym.make(task2_id)\n",
    "        self.switch_interval = switch_interval\n",
    "        self.current_step = 0\n",
    "\n",
    "        # Concatenate observation spaces\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=np.concatenate([self.task1.observation_space.low, self.task2.observation_space.low]),\n",
    "            high=np.concatenate([self.task1.observation_space.high, self.task2.observation_space.high]),\n",
    "            dtype=self.task1.observation_space.dtype\n",
    "        )\n",
    "\n",
    "        # Create a joint action space\n",
    "        print('boom', self.task1.action_space, self.task2.action_space)\n",
    "        self.action_space = gym.spaces.Tuple((self.task1.action_space, self.task2.action_space))\n",
    "\n",
    "        self.task1.reset()\n",
    "        self.task2.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        obs1 = self.task1.reset()\n",
    "        obs2 = self.task2.reset()\n",
    "        concatenated_obs = np.concatenate([obs1, obs2])\n",
    "        return concatenated_obs\n",
    "\n",
    "    def step(self, action):\n",
    "        action1, action2 = action\n",
    "        print('boom',action1,action2)\n",
    "        self.current_step += 1\n",
    "\n",
    "        # Step task1 with action1 (Discrete)\n",
    "        obs1, reward1, done1, info1 = self.task1.step(action1.item())  # Convert action1 tensor to scalar\n",
    "        # Step task2 with action2 (Box)\n",
    "        obs2, reward2, done2, info2 = self.task2.step(action2.tolist())  # Convert action2 tensor to list\n",
    "\n",
    "        concatenated_obs = np.concatenate([obs1, obs2])\n",
    "        reward = reward1 + reward2\n",
    "        done = done1 or done2\n",
    "        info = {**info1, **info2}\n",
    "\n",
    "        if done:\n",
    "            obs1 = self.task1.reset()\n",
    "            obs2 = self.task2.reset()\n",
    "            concatenated_obs = np.concatenate([obs1, obs2])\n",
    "\n",
    "        return concatenated_obs, reward, done, info\n",
    "\n",
    "\n",
    "\n",
    "# Define the custom MLP policy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Categorical, Normal\n",
    "\n",
    "class MLPPolicy(nn.Module):\n",
    "    def __init__(self, obs_space, act_space1, act_space2, hidden_sizes=[128, 128], activation=nn.ReLU):\n",
    "        super(MLPPolicy, self).__init__()\n",
    "        self.obs_space = obs_space\n",
    "        self.act_space1 = act_space1\n",
    "        self.act_space2 = act_space2\n",
    "\n",
    "        layers = []\n",
    "        input_size = obs_space.shape[0]\n",
    "        for hidden_size in hidden_sizes:\n",
    "            layers.append(nn.Linear(input_size, hidden_size))\n",
    "            layers.append(activation())\n",
    "            input_size = hidden_size\n",
    "\n",
    "        self.shared_net = nn.Sequential(*layers)\n",
    "\n",
    "        # Policy head for Discrete action space\n",
    "        self.policy_net1 = nn.Linear(hidden_sizes[-1], act_space1.n)\n",
    "\n",
    "        # Policy head for Box action space (mean and log_std)\n",
    "        self.policy_net2_mean = nn.Linear(hidden_sizes[-1], act_space2.shape[0])\n",
    "        self.policy_net2_log_std = nn.Linear(hidden_sizes[-1], act_space2.shape[0])\n",
    "\n",
    "        # Value network\n",
    "        self.value_net = nn.Sequential(\n",
    "            nn.Linear(obs_space.shape[0], hidden_sizes[0]),\n",
    "            activation(),\n",
    "            nn.Linear(hidden_sizes[0], 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        shared_features = self.shared_net(x)\n",
    "        policy_logits1 = self.policy_net1(shared_features)\n",
    "        policy_mean2 = self.policy_net2_mean(shared_features)\n",
    "        policy_log_std2 = self.policy_net2_log_std(shared_features)\n",
    "        value = self.value_net(x)\n",
    "        return policy_logits1, policy_mean2, policy_log_std2, value\n",
    "\n",
    "    def act(self, x):\n",
    "        policy_logits1, policy_mean2, policy_log_std2, value = self.forward(x)\n",
    "        probs1 = torch.softmax(policy_logits1, dim=-1)\n",
    "        dist1 = Categorical(probs1)\n",
    "        action1 = dist1.sample()\n",
    "        log_prob1 = dist1.log_prob(action1)\n",
    "\n",
    "        std2 = policy_log_std2.exp()\n",
    "        dist2 = Normal(policy_mean2, std2)\n",
    "        action2 = dist2.sample()\n",
    "        log_prob2 = dist2.log_prob(action2).sum(dim=-1)  # Sum log probs for multi-dimensional actions\n",
    "\n",
    "        return (action1, action2), (log_prob1, log_prob2), value\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define the PPO agent\n",
    "class PPOAgent:\n",
    "    def __init__(self, policy, lr=3e-4, gamma=0.99, k_epochs=4, eps_clip=0.2, device='cuda'):\n",
    "        self.policy = policy.to(device)\n",
    "        self.optimizer = optim.Adam(policy.parameters(), lr=lr)\n",
    "        self.gamma = gamma\n",
    "        self.k_epochs = k_epochs\n",
    "        self.eps_clip = eps_clip\n",
    "        self.device = device\n",
    "\n",
    "    def compute_returns(self, rewards, masks, values, next_value):\n",
    "        returns = []\n",
    "        R = next_value\n",
    "        for step in reversed(range(len(rewards))):\n",
    "            R = rewards[step] + self.gamma * R * masks[step]\n",
    "            returns.insert(0, R)\n",
    "        return returns\n",
    "\n",
    "    def update(self, memory):\n",
    "        states, actions, log_probs, rewards, masks, values = memory\n",
    "\n",
    "        states = torch.stack(states).to(self.device)\n",
    "        actions = torch.tensor(actions).to(self.device)\n",
    "        log_probs = torch.tensor(log_probs).to(self.device)\n",
    "        values = torch.stack(values).to(self.device)\n",
    "\n",
    "        next_value = self.policy(states[-1].unsqueeze(0))[1]\n",
    "        returns = self.compute_returns(rewards, masks, values, next_value)\n",
    "        returns = torch.tensor(returns).detach().to(self.device)\n",
    "\n",
    "        old_states = states.detach()\n",
    "        old_actions = actions.detach()\n",
    "        old_log_probs = log_probs.detach()\n",
    "\n",
    "        for _ in range(self.k_epochs):\n",
    "            logits, state_values = self.policy(old_states)\n",
    "            dist = Categorical(torch.softmax(logits, dim=-1))\n",
    "            new_log_probs = dist.log_prob(old_actions)\n",
    "            entropy = dist.entropy().mean()\n",
    "\n",
    "            advantages = returns - state_values.squeeze().detach()\n",
    "            ratio = torch.exp(new_log_probs - old_log_probs)\n",
    "            surr1 = ratio * advantages\n",
    "            surr2 = torch.clamp(ratio, 1 - self.eps_clip, 1 + self.eps_clip) * advantages\n",
    "            actor_loss = -torch.min(surr1, surr2).mean()\n",
    "            critic_loss = nn.MSELoss()(state_values.squeeze(), returns)\n",
    "\n",
    "            loss = 0.5 * critic_loss + actor_loss - 0.01 * entropy\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "\n",
    "# Training loop\n",
    "def train(env, agent, num_episodes=10, batch_size=32):\n",
    "    memory = deque(maxlen=batch_size)\n",
    "    episode_rewards = []\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        #env.reset()\n",
    "        state = torch.FloatTensor(state).to(agent.device).unsqueeze(0)\n",
    "        episode_reward = 0\n",
    "\n",
    "        for t in range(1, 100):\n",
    "            #env.reset()\n",
    "            action, log_prob, value = agent.policy.act(state)\n",
    "            #action = action.item()  # Convert action tensor to scalar\n",
    "            #print(action)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            next_state = torch.FloatTensor(next_state).to(agent.device).unsqueeze(0)\n",
    "\n",
    "            memory.append((state, action, log_prob.item(), reward, 1 - int(done), value))\n",
    "            episode_reward += reward\n",
    "\n",
    "            if len(memory) == batch_size:\n",
    "                states, actions, log_probs, rewards, masks, values = zip(*memory)\n",
    "                agent.update((states, actions, log_probs, rewards, masks, values))\n",
    "                memory.clear()\n",
    "\n",
    "            state = next_state\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        episode_rewards.append(episode_reward)\n",
    "        if (episode + 1) % 10 == 0:\n",
    "            print(f\"Episode {episode + 1}, Reward: {episode_reward}\")\n",
    "\n",
    "    return episode_rewards\n",
    "\n",
    "\n",
    "# Main execution\n",
    "task1_id = 'PerceptualDecisionMaking-v0'\n",
    "task2_id = 'ReachingDelayResponse-v0'\n",
    "switch_interval = 100\n",
    "env = MultiTaskEnv(task1_id, task2_id, switch_interval)\n",
    "\n",
    "obs_space = env.observation_space\n",
    "act_space = env.action_space\n",
    "print(act_space)\n",
    "\n",
    "policy = MLPPolicy(obs_space, act_space[0],act_space[1])\n",
    "agent = PPOAgent(policy)\n",
    "\n",
    "episode_rewards = train(env, agent)\n",
    "\n",
    "# Evaluate the model\n",
    "state = env.reset()\n",
    "state = torch.FloatTensor(state).to(agent.device).unsqueeze(0)\n",
    "for _ in range(1000):\n",
    "    action, _, _ = policy.act(state)\n",
    "    action = action # Convert action tensor to scalar\n",
    "    #print(env)\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    next_state = torch.FloatTensor(next_state).to(agent.device).unsqueeze(0)\n",
    "    #env.render()\n",
    "    state = next_state\n",
    "    if done:\n",
    "        state = env.reset()\n",
    "        state = torch.FloatTensor(state).to(agent.device).unsqueeze(0)\n",
    "\n",
    "env.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
