{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "()\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "must be real number, not dict",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 146\u001b[0m\n\u001b[1;32m    144\u001b[0m agent\u001b[38;5;241m.\u001b[39mreset_hidden()  \u001b[38;5;66;03m# Reset hidden state at the beginning of each episode\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[0;32m--> 146\u001b[0m     hidden_state \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mq_network\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Preprocess the state through RNN\u001b[39;00m\n\u001b[1;32m    147\u001b[0m     action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mchoose_action(hidden_state)  \u001b[38;5;66;03m# Choose action based on epsilon-greedy policy\u001b[39;00m\n\u001b[1;32m    148\u001b[0m     next_state, reward, done, truncated, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)  \u001b[38;5;66;03m# Take the action\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 116\u001b[0m, in \u001b[0;36mpreprocess_state\u001b[0;34m(state, rnn_q_network, device)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03mPreprocesses the state received from the environment by passing it through the RNNQNetwork to obtain the hidden state.\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;124;03m- hidden_state: Hidden state generated by passing the state through the RNN\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# Convert state to torch tensor and move to device\u001b[39;00m\n\u001b[0;32m--> 116\u001b[0m state_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;66;03m# Initialize hidden state for the first batch item\u001b[39;00m\n\u001b[1;32m    119\u001b[0m hidden \u001b[38;5;241m=\u001b[39m rnn_q_network\u001b[38;5;241m.\u001b[39minit_hidden(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mTypeError\u001b[0m: must be real number, not dict"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn \n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import gymnasium as gym\n",
    "\n",
    "class RNNQNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(RNNQNetwork, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.i2h = nn.Linear(input_dim, hidden_dim)\n",
    "        self.h2h = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.h2o = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        x = torch.tensor(x, dtype=torch.float32).unsqueeze(0)  # Ensure x is a tensor and unsqueeze for batch dimension\n",
    "        hidden = torch.tanh(self.i2h(x) + self.h2h(hidden))\n",
    "        output = self.h2o(hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(batch_size, self.hidden_dim)\n",
    "    \n",
    "class DQNAgent:\n",
    "    def __init__(self, env, hidden_dim=64, learning_rate=0.001, discount_factor=0.99, epsilon=1.0, epsilon_min=0.01, epsilon_decay=0.995, buffer_size=10000, batch_size=64):\n",
    "        self.env = env\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.buffer_size = buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.q_network = RNNQNetwork(env.observation_space.n, hidden_dim, env.action_space.n).to(self.device)  # Use shape[0] for input_dim\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=learning_rate)\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "        self.hidden = self.q_network.init_hidden(1).to(self.device)\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        q_values, self.hidden = self.q_network(state, self.hidden)\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return self.env.action_space.sample()  # Explore\n",
    "        else:\n",
    "            return torch.argmax(q_values).item()  # Exploit\n",
    "\n",
    "    def store_transition(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def learn(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        # Convert lists to numpy arrays and concatenate\n",
    "        states = np.array(states, dtype=np.float32)\n",
    "        actions = np.array(actions, dtype=np.int64)\n",
    "        rewards = np.array(rewards, dtype=np.float32)\n",
    "        next_states = np.array(next_states, dtype=np.float32)\n",
    "        dones = np.array(dones, dtype=np.float32)\n",
    "\n",
    "        # Convert concatenated numpy arrays to PyTorch tensors\n",
    "        states_tensor = torch.tensor(states).to(self.device)\n",
    "        actions_tensor = torch.tensor(actions).unsqueeze(-1).to(self.device)\n",
    "        rewards_tensor = torch.tensor(rewards).unsqueeze(-1).to(self.device)\n",
    "        next_states_tensor = torch.tensor(next_states).to(self.device)\n",
    "        dones_tensor = torch.tensor(dones).unsqueeze(-1).to(self.device)\n",
    "\n",
    "        # Initialize hidden state for the first batch item\n",
    "        #self.hidden = self.q_network.init_hidden(self.batch_size).to(self.device)\n",
    "\n",
    "        current_q_values, _ = self.q_network(states_tensor, self.hidden)\n",
    "        current_q_values = current_q_values.gather(1, actions_tensor)\n",
    "        next_q_values, _ = self.q_network(next_states_tensor, self.hidden)  # Use the same hidden state for next states\n",
    "        next_q_values = next_q_values.max(1)[0].unsqueeze(-1)\n",
    "        target_q_values = rewards_tensor + self.discount_factor * next_q_values * (1 - dones_tensor)\n",
    "\n",
    "        loss = self.criterion(current_q_values, target_q_values)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "    def reset_hidden(self):\n",
    "        self.hidden = self.q_network.init_hidden(1).to(self.device)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def preprocess_state(state, rnn_q_network, device):\n",
    "    \"\"\"\n",
    "    Preprocesses the state received from the environment by passing it through the RNNQNetwork to obtain the hidden state.\n",
    "    \n",
    "    Parameters:\n",
    "    - state: Current state from the environment\n",
    "    - rnn_q_network: RNNQNetwork model instance\n",
    "    - device: Device (cpu or cuda) on which to run the computation\n",
    "    \n",
    "    Returns:\n",
    "    - hidden_state: Hidden state generated by passing the state through the RNN\n",
    "    \"\"\"\n",
    "    # Convert state to torch tensor and move to device\n",
    "    state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Initialize hidden state for the first batch item\n",
    "    hidden = rnn_q_network.init_hidden(1).to(device)\n",
    "    \n",
    "    # Pass state through the RNNQNetwork\n",
    "    with torch.no_grad():\n",
    "        _, hidden_state = rnn_q_network(state_tensor, hidden)\n",
    "    \n",
    "    return hidden_state.squeeze(0).cpu().numpy() \n",
    "\n",
    "from environment import DelaySampleToMatchEnv\n",
    "\n",
    "#env=DelaySampleToMatchEnv(5)\n",
    "env = gym.make('FrozenLake-v1', desc=None, map_name=\"4x4\", is_slippery=False)\n",
    "# Instantiate the agent\n",
    "print(env.observation_space.shape)\n",
    "agent = DQNAgent(env)\n",
    "\n",
    "n_episodes = 2500\n",
    "win_pct_list = []\n",
    "scores = []\n",
    "\n",
    "\n",
    "for i in range(n_episodes):\n",
    "    state = env.reset()  # Reset the environment\n",
    "    done = False\n",
    "    score = 0\n",
    "    agent.reset_hidden()  # Reset hidden state at the beginning of each episode\n",
    "    while not done:\n",
    "        hidden_state = preprocess_state(state, agent.q_network, agent.device)  # Preprocess the state through RNN\n",
    "        action = agent.choose_action(hidden_state)  # Choose action based on epsilon-greedy policy\n",
    "        next_state, reward, done, truncated, info = env.step(action)  # Take the action\n",
    "        agent.store_transition(state, action, reward, next_state, done)\n",
    "        agent.learn()  # Update Q-network\n",
    "        state = next_state  # Move to the next state\n",
    "        score += reward\n",
    "    scores.append(score)\n",
    "    if i % 100 == 0:\n",
    "        avg_score = np.mean(scores[-100:])\n",
    "        win_pct_list.append(avg_score)\n",
    "        print('episode', i, 'win pct %.2f' % avg_score, 'epsilon %.2f' % agent.epsilon)\n",
    "\n",
    "# Plotting the win percentage over episodes\n",
    "plt.plot(win_pct_list)\n",
    "plt.xlabel('Episodes (x100)')\n",
    "plt.ylabel('Win Percentage')\n",
    "plt.title('Win Percentage over Time')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
